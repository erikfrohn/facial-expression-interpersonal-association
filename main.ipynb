{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main working file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "## system\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "## data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "data-in\\09_10\\avi\\Camera_pp10_navigator_0_20240703_1008.avi\n",
      "Rewrapping video\n",
      "Video was already rewrapped\n",
      "data-in\\09_10\\avi\\Camera_pp10_navigator_0_20240703_1008_rewrapped.avi\n",
      "Video already has metadata\n",
      "data-in\\09_10\\avi\\Camera_pp9_pilot_0_20240703_1008.avi\n",
      "Video already has metadata\n",
      "data-in\\21_22\\avi\\Camera_pp21_pilot_0_20240925_1339.avi\n",
      "Video already has metadata\n",
      "data-in\\21_22\\avi\\Camera_pp22_navigator_0_20240925_1338.avi\n",
      "Rewrapping video\n"
     ]
    }
   ],
   "source": [
    "# Import the function\n",
    "from util.video_transformation import rewrap_video\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "base_path = \"data-in\"\n",
    "input_folder = \"avi\"\n",
    "\n",
    "for team_folder in os.listdir(base_path):\n",
    "    if team_folder not in ['09_10', '21_22']:\n",
    "        continue\n",
    "    path_videos = os.path.join(base_path, team_folder, input_folder, \"*.avi\")\n",
    "    files = glob.glob(path_videos)\n",
    "\n",
    "    for file in files:\n",
    "        rewrap_video(file, os.path.join(base_path, team_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge audio and video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Action Units (AUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp05_navigator_instructional_video_0.csv has already been processed. Continuing...\n",
      "pp05_navigator_discussion_phase_0.csv has already been processed. Continuing...\n",
      "pp05_navigator_discussion_phase_1.csv has already been processed. Continuing...\n",
      "pp06_pilot_instructional_video_0.csv has already been processed. Continuing...\n",
      "pp06_pilot_discussion_phase_0.csv has already been processed. Continuing...\n",
      "pp06_pilot_discussion_phase_1.csv has already been processed. Continuing...\n",
      "pp07_navigator_instructional_video_0.csv has already been processed. Continuing...\n",
      "pp07_navigator_discussion_phase_0.csv has already been processed. Continuing...\n",
      "pp07_navigator_discussion_phase_1.csv has already been processed. Continuing...\n",
      "pp08_pilot_instructional_video_0.csv has already been processed. Continuing...\n",
      "pp08_pilot_discussion_phase_0.csv has already been processed. Continuing...\n",
      "pp08_pilot_discussion_phase_1.csv has already been processed. Continuing...\n",
      "pp09_navigator_instructional_video_0.csv has already been processed. Continuing...\n",
      "pp09_navigator_discussion_phase_0.csv has already been processed. Continuing...\n",
      "pp09_navigator_discussion_phase_1.csv has already been processed. Continuing...\n",
      "pp10_pilot_instructional_video_0.csv has already been processed. Continuing...\n",
      "pp10_pilot_discussion_phase_0.csv has already been processed. Continuing...\n",
      "pp10_pilot_discussion_phase_1.csv has already been processed. Continuing...\n"
     ]
    }
   ],
   "source": [
    "from util.feature_extraction import extract_features\n",
    "data_in = r'data-in'\n",
    "data_out = r'data-out'\n",
    "\n",
    "for pair in os.listdir(data_in):\n",
    "    input_folder = os.path.join(data_in, pair)\n",
    "    output_folder = os.path.join(data_out, pair, 'au')\n",
    "    for participant in pair.split('_'):\n",
    "        # TODO: handle missing data (NaN or recovered)\n",
    "        aus = extract_features(input_folder, participant, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = r\"data-out\" \n",
    "for pair in os.listdir(location):\n",
    "    filename = os.path.join(location, pair, \"selection\")\n",
    "    os.makedirs(filename, exist_ok=True)\n",
    "    filename = os.path.join(location, pair, \"extraction\")\n",
    "    os.makedirs(filename, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.feature_selection as fs\n",
    "location = r\"data-out\" \n",
    "names = ['discussion_phase_0', 'discussion_phase_1', 'instructional_video_0']\n",
    "for pair in os.listdir(location):\n",
    "    # TODO: make it check which pair the file is from and make two dataframes of that. Then combine\n",
    "    for file in os.listdir(os.path.join(location, pair, \"au\")):\n",
    "        if \".csv\" in file: \n",
    "            filename = os.path.join(location, pair, \"au\", file)\n",
    "            participant, _ = file.split(\"_\",1)\n",
    "            df = pd.read_csv(filename)\n",
    "            for name in names:\n",
    "                if name in file:\n",
    "                    factors = fs.au_to_factors(df)\n",
    "                    factors.to_csv(os.path.join(location, pair, \"selection\", f\"{participant}_{name}_factors.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files of pairs in 05_06 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "(17, 17)\n",
      "files of pairs in 07_08 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "(17, 17)\n",
      "(17, 17)\n"
     ]
    }
   ],
   "source": [
    "import util.feature_selection as fs\n",
    "\n",
    "# corrCA takes a df as input. This df should be all files for a pair. \n",
    "location = r\"data-out\" \n",
    "for pair in os.listdir(location):\n",
    "    filename = os.path.join(location, pair)\n",
    "    data = {}\n",
    "    nav, pil = pair.split(\"_\")\n",
    "    nav_df = pd.DataFrame()\n",
    "    pil_df = pd.DataFrame()\n",
    "    for file in os.listdir(os.path.join(filename, \"au\")):\n",
    "        if \".csv\" in file: \n",
    "            df = pd.read_csv(os.path.join(filename, \"au\", file))\n",
    "            if \".csv\" in file and nav in file:\n",
    "                nav_df = pd.concat([nav_df, df])\n",
    "            if \".csv\" in file and pil in file:\n",
    "                pil_df = pd.concat([pil_df, df])\n",
    "    nav_df, pil_df = fs.make_equal_length(pair, nav_df, pil_df)\n",
    "    w = fs.corrCA_weights(nav_df, pil_df) #output = pair/corrca.csv\n",
    "    w.to_csv(os.path.join(filename, f\"{pair}_corrca_weights.csv\"), index=False)\n",
    "\n",
    "names = ['discussion_phase_0', 'discussion_phase_1', 'instructional_video_0']\n",
    "for pair in os.listdir(location):\n",
    "    for file in os.listdir(os.path.join(location, pair, \"au\")):\n",
    "        if \".csv\" in file: \n",
    "            filename = os.path.join(location, pair, \"au\", file)\n",
    "            participant, _ = file.split(\"_\", 1)\n",
    "            df = pd.read_csv(filename)\n",
    "            for name in names:\n",
    "                if name in file:\n",
    "                    w = pd.read_csv(os.path.join(location, pair, f'{pair}_corrca_weights.csv'))\n",
    "                    corrca = fs.apply_corrCA_weights(df, w)\n",
    "                    corrca.to_csv(os.path.join(location, pair, \"selection\", f\"{participant}_{name}_corrca.csv\"), index=False)\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files of pairs in 05_06_discussion_phase_0 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 05_06_discussion_phase_0 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 07_08_instructional_video_0 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 07_08_instructional_video_0 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 07_08_discussion_phase_0 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 07_08_discussion_phase_0 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 07_08_discussion_phase_1 do not have the same amount of datapoints - temporary fix has made them equal length\n",
      "files of pairs in 07_08_discussion_phase_1 do not have the same amount of datapoints - temporary fix has made them equal length\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import util.feature_selection as fs\n",
    "\n",
    "location = r\"data-out\" \n",
    "sets = ['corrca', 'factors']\n",
    "phases = ['instructional_video_0', 'discussion_phase_0', 'discussion_phase_1']\n",
    "factors = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6']\n",
    "\n",
    "# each pair gets a separate file\n",
    "for pair in os.listdir(location):\n",
    "    df = pd.DataFrame()\n",
    "    nav, pil = pair.split(\"_\")\n",
    "    file = os.path.join(location, pair, 'selection')\n",
    "\n",
    "    # format:  | factor1 | factor2 | ... | factor 6 | corrca | (per phase)\n",
    "    for phase in phases:\n",
    "        correlation = []\n",
    "        # factors\n",
    "        nav_factors = pd.read_csv(os.path.join(file, f\"pp{nav}_{phase}_factors.csv\"))\n",
    "        pil_factors = pd.read_csv(os.path.join(file, f\"pp{pil}_{phase}_factors.csv\"))\n",
    "        nav_factors, pil_factors = fs.make_equal_length(f\"{pair}_{phase}\", nav_factors, pil_factors)\n",
    "        for f in factors:\n",
    "            corr, _ = pearsonr(nav_factors[f].values, pil_factors[f].values)\n",
    "            correlation.append(corr)\n",
    "\n",
    "        # corrca\n",
    "        nav_corrca = pd.read_csv(os.path.join(file, f\"pp{nav}_{phase}_corrca.csv\"))\n",
    "        pil_corrca = pd.read_csv(os.path.join(file, f\"pp{pil}_{phase}_corrca.csv\"))\n",
    "        nav_corrca, pil_corrca = fs.make_equal_length(f\"{pair}_{phase}\", nav_corrca, pil_corrca)\n",
    "        \n",
    "        corr, _ = pearsonr(nav_corrca['component1'].values, pil_corrca['component1'].values)\n",
    "        correlation.append(corr)\n",
    "        \n",
    "        df[phase] = correlation\n",
    "    df.to_csv(os.path.join(location, pair, \"extraction\", f\"{pair}_pearson.csv\"))\n",
    "    df.to_csv(os.path.join(\"results\", f\"{pair}_pearson.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cRQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
