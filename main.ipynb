{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main working file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "## system\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import util.feature_selection as fs\n",
    "import util.correlation_measure as cm\n",
    "\n",
    "LOCATION = \"data\"\n",
    "ACTION_UNITS = \"data/au\"\n",
    "FEATURE_FOLDER = \"features\"\n",
    "CORRELATION_FOLDER = 'correlations'\n",
    "PHASES = [f'{name}_{i}' for name, num in  [(\"instructional_video\", 1), (\"discussion_phase\", 2), ('reschu_run',8)] for i in range(num)]#, (\"reschu_run\", 8)] for i in range(num)]\n",
    "SETS = ['corrca', 'factors']\n",
    "FACTORS = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6']\n",
    "COMPONENTS = ['c1', 'c2', 'c3']\n",
    "PAIRS = [f'0{i}_0{i+1}' for i in np.arange(1,9,2)]\n",
    "PAIRS.append(\"09_10\")\n",
    "PAIRS.extend([f'{i}_{i+1}' for i in np.arange(11,104,2)])\n",
    "\n",
    "\n",
    "AVAILABLE_PAIRS = []\n",
    "SKIP_PAIRS = ['53_54', '55_56', \"63_64\", \"89_90\"]\n",
    "for file in os.listdir(ACTION_UNITS):\n",
    "    if \".csv\" in file and 'Data' in file: \n",
    "        filename = os.path.join(ACTION_UNITS, file)\n",
    "        participant, _ = file.split(\"_\",1)\n",
    "        participant = participant[2:]\n",
    "        for p in PAIRS:\n",
    "            p1,p2 = p.split(\"_\")\n",
    "            if participant == p1 or participant == p2:\n",
    "                pair = p\n",
    "                break\n",
    "        if pair in SKIP_PAIRS:\n",
    "            continue\n",
    "        if pair not in AVAILABLE_PAIRS:\n",
    "            AVAILABLE_PAIRS.append(pair)\n",
    "            dir = os.path.join(LOCATION, pair)\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            dir = os.path.join(LOCATION, pair, FEATURE_FOLDER)\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            dir = os.path.join(LOCATION, pair, CORRELATION_FOLDER)\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "print(len(AVAILABLE_PAIRS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05_06\n",
      "Already computed corrca weights for 05_06, skipping...\n",
      "07_08\n",
      "Already computed corrca weights for 07_08, skipping...\n",
      "09_10\n",
      "Already computed corrca weights for 09_10, skipping...\n",
      "103_104\n",
      "Already computed corrca weights for 103_104, skipping...\n",
      "27_28\n",
      "Already computed corrca weights for 27_28, skipping...\n",
      "83_84\n",
      "Already computed corrca weights for 83_84, skipping...\n",
      "85_86\n",
      "Already computed corrca weights for 85_86, skipping...\n",
      "87_88\n",
      "Already computed corrca weights for 87_88, skipping...\n",
      "91_92\n",
      "Already computed corrca weights for 91_92, skipping...\n",
      "93_94\n",
      "Already computed corrca weights for 93_94, skipping...\n",
      "95_96\n",
      "Already computed corrca weights for 95_96, skipping...\n",
      "97_98\n",
      "Already computed corrca weights for 97_98, skipping...\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "## CREATING WEIGHTS FOR EACH PAIR SEPARATELY\n",
    "\n",
    "# corrCA takes a df as input. This df should be all files for a pair. \n",
    "for pair in AVAILABLE_PAIRS:\n",
    "    print(pair)\n",
    "    weights_path = os.path.join(LOCATION, pair, f'{pair}_corrca_weights.csv')\n",
    "    if os.path.exists(weights_path):\n",
    "        print(f\"Already computed corrca weights for {pair}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    p1, p2 = pair.split(\"_\")\n",
    "    p1_df = pd.DataFrame()\n",
    "    p2_df = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(ACTION_UNITS):\n",
    "        if '.csv' in file and 'Data' in file and p1 in file: \n",
    "            df = pd.read_csv(os.path.join(ACTION_UNITS, file))\n",
    "            p1_df = pd.concat([p1_df, df])\n",
    "        if '.csv' in file and 'Data' in file and p2 in file: \n",
    "            df = pd.read_csv(os.path.join(ACTION_UNITS, file))\n",
    "            p2_df = pd.concat([p2_df, df])\n",
    "    if len(p1_df) < 1 or len(p2_df) < 1:\n",
    "        print(f\"Pair {pair} misses either navigator or pilot action unit files, REMOVING FROM AVAILABLE PAIRS...\")\n",
    "        AVAILABLE_PAIRS = [i for i in AVAILABLE_PAIRS if i!=pair]\n",
    "        continue\n",
    "    p1_df = p1_df.iloc[:len(p2_df)]\n",
    "    p2_df = p2_df.iloc[:len(p1_df)]\n",
    "\n",
    "    mask = (p1_df.isna().any(axis=1).values) | (p2_df.isna().any(axis=1).values)\n",
    "    p1_df = p1_df[~mask].reset_index(drop=True)\n",
    "    p2_df = p2_df[~mask].reset_index(drop=True)\n",
    "    w = fs.corrCA_weights(p1_df, p2_df) #output = pair/pair_corrca_weights.csv\n",
    "    w.to_csv(os.path.join(LOCATION, pair, f\"{pair}_corrca_weights.csv\"), index=False)\n",
    "\n",
    "print(len(AVAILABLE_PAIRS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRACTING FACIAL FACTORS AND CORRCA FOR EACH FILE SEPARATELY\n",
    "for pair in AVAILABLE_PAIRS:\n",
    "    # check whether we can perform corrca\n",
    "    if not os.path.exists(os.path.join(LOCATION, pair, f'{pair}_corrca_weights.csv')):\n",
    "        print(f\"Found no corrca weights for {pair}\")    \n",
    "    else:\n",
    "        w = pd.read_csv(os.path.join(LOCATION, pair, f'{pair}_corrca_weights.csv'))\n",
    "\n",
    "    p1, p2 = pair.split(\"_\")\n",
    "    for phase in PHASES: \n",
    "        p1_file = os.path.join(ACTION_UNITS, f\"pp{p1}_navigator_{phase}_AU_withMissingData.csv\")\n",
    "        p2_file = os.path.join(ACTION_UNITS, f\"pp{p2}_pilot_{phase}_AU_withMissingData.csv\")\n",
    "\n",
    "        if os.path.exists(p1_file) and os.path.exists(p2_file):\n",
    "            p1_df = pd.read_csv(p1_file)\n",
    "            p2_df = pd.read_csv(p2_file)\n",
    "\n",
    "            # make equal length\n",
    "            p1_df = p1_df.iloc[:len(p2_df)]\n",
    "            p2_df = p2_df.iloc[:len(p1_df)]\n",
    "\n",
    "            # remove all missing data\n",
    "            mask = (p1_df.isna().any(axis=1).values) | (p2_df.isna().any(axis=1).values)\n",
    "            p1_df = p1_df[~mask].reset_index(drop=True)\n",
    "            p2_df = p2_df[~mask].reset_index(drop=True)\n",
    "\n",
    "            p1_factors = fs.au_to_factors(p1_df)\n",
    "            p1_factors.to_csv(os.path.join(LOCATION, pair, FEATURE_FOLDER, f\"pp{p1}_{phase}_factors.csv\"), index=False)\n",
    "            \n",
    "            p2_factors = fs.au_to_factors(p2_df)\n",
    "            p2_factors.to_csv(os.path.join(LOCATION, pair, FEATURE_FOLDER, f\"pp{p2}_{phase}_factors.csv\"), index=False)\n",
    "\n",
    "            if not os.path.exists(os.path.join(LOCATION, pair, f'{pair}_corrca_weights.csv')):\n",
    "                continue\n",
    "\n",
    "            p1_corrca = fs.apply_corrCA_weights(p1_df, w)\n",
    "            p1_corrca.to_csv(os.path.join(LOCATION, pair, FEATURE_FOLDER, f\"{p1}_{phase}_corrca.csv\"), index=False)\n",
    "\n",
    "            p2_corrca = fs.apply_corrCA_weights(p2_df, w)\n",
    "            p2_corrca.to_csv(os.path.join(LOCATION, pair, FEATURE_FOLDER, f\"{p2}_{phase}_corrca.csv\"), index=False)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation measure: cRQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cRQA at different time lags for everything\n",
    "# TODO: RR at diag 0 for synchrony\n",
    "# TODO: RR max lag for mimicry\n",
    "\n",
    "# chosen radii that work to get meaningful results: real pairs are still captured and fake pairs are not\n",
    "RADII = {\n",
    "    'f1': 0.5,\n",
    "    'f2': 0.3,\n",
    "    'f3': 0.5,\n",
    "    'f4': 0.5,\n",
    "    'f5': 0.5,\n",
    "    'f6': 0.5,\n",
    "    'c1': 0.1,\n",
    "    'c2': 0.1,\n",
    "    'c3': 0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REAL PAIRS - FACTORS\n",
      "\n",
      "Processing factor f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erik\\AppData\\Local\\Temp\\ipykernel_13948\\4012450582.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing factor f2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Initialize DataFrame to store all results\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'pair', 'phase', 'method', 'component_factor', 'radius', \n",
    "    'non_event_matches', 'condition', 'RR'\n",
    "])\n",
    "\n",
    "# Define radii to test\n",
    "radii = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "def process_analysis(p1_df, p2_df, component, radius, remove_non_events, method, pair, phase, condition):\n",
    "    output = cm.crqa_lag_analysis(\n",
    "        p1_df[component].values, \n",
    "        p2_df[component].values, \n",
    "        radius=radius,\n",
    "        remove_non_event_matches=remove_non_events\n",
    "    )\n",
    "    \n",
    "    new_row = {\n",
    "        'pair': pair,\n",
    "        'phase': phase,\n",
    "        'method': method,\n",
    "        'component_factor': component,\n",
    "        'radius': radius,\n",
    "        'non_event_matches': 'excluded' if remove_non_events else 'included',\n",
    "        'condition': condition,\n",
    "        'RR': output['RR']\n",
    "    }\n",
    "    \n",
    "    return new_row\n",
    "\n",
    "# Process real pairs - Factors\n",
    "print(\"\\nREAL PAIRS - FACTORS\\n\")\n",
    "for f in FACTORS:\n",
    "    print(f\"Processing factor {f}\")\n",
    "    for r in radii:\n",
    "        for pair in AVAILABLE_PAIRS[:10]:\n",
    "            p1, p2 = pair.split(\"_\")\n",
    "            for phase in PHASES:\n",
    "                p1_loc = os.path.join(LOCATION, pair, FEATURE_FOLDER, f'pp{p1}_{phase}_factors.csv')\n",
    "                p2_loc = os.path.join(LOCATION, pair, FEATURE_FOLDER, f'pp{p2}_{phase}_factors.csv')\n",
    "                \n",
    "                if os.path.exists(p1_loc) and os.path.exists(p2_loc):\n",
    "                    p1_df = pd.read_csv(p1_loc)\n",
    "                    p2_df = pd.read_csv(p2_loc)\n",
    "                    \n",
    "                    # With non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, f, r, False, \n",
    "                        'factor', pair, phase, 'real'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    \n",
    "                    # Without non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, f, r, True, \n",
    "                        'factor', pair, phase, 'real'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Process real pairs - CORRCA\n",
    "print(\"\\nREAL PAIRS - CORRCA\\n\")\n",
    "for c in COMPONENTS:\n",
    "    print(f\"Processing component {c}\")\n",
    "    for r in radii:\n",
    "        for pair in AVAILABLE_PAIRS[:10]:\n",
    "            p1, p2 = pair.split(\"_\")\n",
    "            for phase in PHASES:\n",
    "                p1_loc = os.path.join(LOCATION, pair, FEATURE_FOLDER, f'pp{p1}_{phase}_corrca.csv')\n",
    "                p2_loc = os.path.join(LOCATION, pair, FEATURE_FOLDER, f'pp{p2}_{phase}_corrca.csv')\n",
    "                \n",
    "                if os.path.exists(p1_loc) and os.path.exists(p2_loc):\n",
    "                    p1_df = pd.read_csv(p1_loc)\n",
    "                    p2_df = pd.read_csv(p2_loc)\n",
    "                    \n",
    "                    # With non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, c, r, False, \n",
    "                        'corrca', pair, phase, 'real'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    \n",
    "                    # Without non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, c, r, True, \n",
    "                        'corrca', pair, phase, 'real'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Process surrogate pairs - Factors\n",
    "print(\"\\nSURROGATE PAIRS - FACTORS\\n\")\n",
    "index_real = np.arange(10)\n",
    "index_fake = np.append(np.arange(1,10),0)\n",
    "\n",
    "for f in FACTORS:\n",
    "    print(f\"Processing factor {f} for surrogate pairs\")\n",
    "    for r in radii:\n",
    "        for i in range(10):\n",
    "            pair1 = AVAILABLE_PAIRS[index_real[i]]\n",
    "            pair2 = AVAILABLE_PAIRS[index_fake[i]]\n",
    "            p1, _ = pair1.split(\"_\")\n",
    "            _, p2 = pair2.split(\"_\")\n",
    "            \n",
    "            for phase in PHASES:\n",
    "                p1_loc = os.path.join(LOCATION, pair1, FEATURE_FOLDER, f'pp{p1}_{phase}_factors.csv')\n",
    "                p2_loc = os.path.join(LOCATION, pair2, FEATURE_FOLDER, f'pp{p2}_{phase}_factors.csv')\n",
    "                \n",
    "                if os.path.exists(p1_loc) and os.path.exists(p2_loc):\n",
    "                    p1_df = pd.read_csv(p1_loc)\n",
    "                    p2_df = pd.read_csv(p2_loc)\n",
    "                    \n",
    "                    # With non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, f, r, False, \n",
    "                        'factor', f\"{pair1}_{pair2}\", phase, 'fake'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    \n",
    "                    # Without non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, f, r, True, \n",
    "                        'factor', f\"{pair1}_{pair2}\", phase, 'fake'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Process surrogate pairs - CORRCA\n",
    "print(\"\\nSURROGATE PAIRS - CORRCA\\n\")\n",
    "for c in COMPONENTS:\n",
    "    print(f\"Processing component {c} for surrogate pairs\")\n",
    "    for r in radii:\n",
    "        for i in range(10):\n",
    "            pair1 = AVAILABLE_PAIRS[index_real[i]]\n",
    "            pair2 = AVAILABLE_PAIRS[index_fake[i]]\n",
    "            p1, _ = pair1.split(\"_\")\n",
    "            _, p2 = pair2.split(\"_\")\n",
    "            \n",
    "            for phase in PHASES:\n",
    "                p1_loc = os.path.join(LOCATION, pair1, FEATURE_FOLDER, f'pp{p1}_{phase}_corrca.csv')\n",
    "                p2_loc = os.path.join(LOCATION, pair2, FEATURE_FOLDER, f'pp{p2}_{phase}_corrca.csv')\n",
    "                \n",
    "                if os.path.exists(p1_loc) and os.path.exists(p2_loc):\n",
    "                    p1_df = pd.read_csv(p1_loc)\n",
    "                    p2_df = pd.read_csv(p2_loc)\n",
    "                    \n",
    "                    # With non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, c, r, False, \n",
    "                        'corrca', f\"{pair1}_{pair2}\", phase, 'fake'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    \n",
    "                    # Without non-event matches\n",
    "                    new_row = process_analysis(\n",
    "                        p1_df, p2_df, c, r, True, \n",
    "                        'corrca', f\"{pair1}_{pair2}\", phase, 'fake'\n",
    "                    )\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('crqa_results_all_pairs.csv', index=False)\n",
    "print(\"Processing complete. Results saved to crqa_results_all_pairs.csv\")\n",
    "\n",
    "# Display sample of the results\n",
    "print(\"\\nSample of the results DataFrame:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
